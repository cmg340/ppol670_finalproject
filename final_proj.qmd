---
title: "Final Project - Predicting Health Indicators"
author: "Elena Spielmann, Patrick Jones, Leanne Chook, and Maeve Grady"
format: 
  html:
    code-fold: true
    self-contained: true
execute: 
  echo: true
  warning: false
  error: false
editor_options: 
  chunk_output_type: console
---
## Introduction 

Obesity has reached epidemic proportions in the United States, and affects people of all ages, socioeconomic backgrounds and ethnicities, which imposes substantial economic burdens in the form of productivity losses, strains to the healthcare system and costs. 

The prevalence of obesity in the United States was approximately 41.9% in 2017 to 2020, of that, the prevalence of severe obesity also increased to 9.2%. This increasing trend is worrying because obesity can lead to several other health conditions such as heart disease, strokes, type 2 diabetes, and certain types of cancers, all of which are the leading causes of preventable and premature deaths in the country. 

Furthermore, this increasing trend of obesity also has other implications on national security, especially as more individuals become ineligible for the military, therefore, significantly reducing military recruitment numbers. 

**Motivating question:**
The main motivating question for this project was to examine obesity rates across the United States and find predictors that can help policymakers make effective decisions to help reduce obesity prevalence in the most vulnerable areas. 

**Outline of project:**
This project will be as follows, first we will examine and clean up the available information from datasets downloaded from PLACES and the Food Access Repository Atlas. Then we will conduct exploratory data analysis (EDA) and geospatial analysis to guide our decision for the final models, and finally we create several predictive models to find the best model with highest predictive power. 

*Setup (loading packages)*

```{r}
#install.packages("httr")
library(httr)
library(sf)
#install.packages("tmap")
library(tmap)
library(dotenv)
library(here)
library(readxl)
library(dplyr)
library(tidyverse)
library(tidymodels)
library(themis)
library(rpart.plot)
library(vip)
library(lubridate)
library(rpart)
library(ranger)
library(ggplot2)
library(parsnip)
library(yardstick)
library(sf)
library(janitor) 
library(stringr)
library(RSocrata)
library(stringr)
library(mice)
library(sp)
library(tidyr)
library(caret)
library(patchwork)

```

## Reading in Data using APIs

```{r}
#accessing FARA data through ArcGIS REST API
## to write this code I consulted this blog post: https://community.esri.com/t5/gis-blog/accessing-arcgis-rest-services-using-r/ba-p/898451

url <- parse_url("https://gis.ers.usda.gov/arcgis/rest/services")
url$path <- paste(url$path, "foodaccess2019/MapServer/0/query", sep = "/")
url$query <- list(returnGeometry = "true",
                  f = "geojson",
                  outFields = "*")
request <- build_url(url)

foodaccess <- st_read(request) ## this should create a full dataframe but is obviously not working right now.  


##using this api to try to figure out how the query should work:
##https://gis.ers.usda.gov/arcgis/rest/services/foodaccess2019/MapServer/0/query?where=&text=&objectIds=&time=&geometry=&geometryType=esriGeometryPolygon&inSR=&spatialRel=esriSpatialRelIntersects&distance=&units=esriSRUnit_Foot&relationParam=&outFields=*&returnGeometry=true&returnTrueCurves=false&maxAllowableOffset=&geometryPrecision=&outSR=&havingClause=&returnIdsOnly=false&returnCountOnly=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&returnZ=false&returnM=false&gdbVersion=&historicMoment=&returnDistinctValues=false&resultOffset=&resultRecordCount=&returnExtentOnly=false&datumTransformation=&parameterValues=&rangeValues=&quantizationParameters=&featureEncoding=esriDefault&f=html

```

```{r}
#install.packages("RSocrata")
library(RSocrata) ## the Rsocrata package has to be loaded in after httr package for the above code to work 

##reading in api credentials
load_dot_env(here(".env"))
app_token <- Sys.getenv("PLACES_app_token")
user_name <- Sys.getenv("PLACES_username")
password <- Sys.getenv("PLACES_password")

#PLACES dataset

places <- read.socrata(
  "https://chronicdata.cdc.gov/resource/swc5-untb.json",
  app_token = paste(app_token),
  email     = paste(user_name),
  password  = paste(password)
)
```

## Combining the Datasets

**The Food Access Research Atlas (FARA)** provides a variety of food access measures for low income and low access census tracts. Food access measures use income, transportation, and distance from grocery stores and other food sellers to determine how accessible food is to residents of a given census tract. This dataset includes distinct measures of access for urban and rural populations.

(rewrite this)
**PLACES** is an expansion of the original 500 Cities Project that began in 2015. The original project was launched by the Centers for Disease Control and Prevention (CDC) in partnership with the Robert Wood Johnson Foundation (RWJF) and CDC Foundation. In 2018, this partnership was extended through 2020. In 2020, the project expanded to provide small area estimates (SAE) for counties, places, census tracts, and ZIP Code Tabulation Areas (ZCTA) across the entire United States.

```{r}
# load the FARA dataset 
fara <- read_excel("fara_2019.xlsx")

# clean the variable names 
fara <- clean_names(fara)

# load the PLACES dataset
places <- st_read("PLACES_ Local Data for Better Health, Census Tract Data 2022 release.geojson")

places <- places %>%
  rename(census_tract = locationid)

#The places dataset must be made tidy in order to properly merge it with FARA
#Column names will come from measureid, and values from data_value
#Crude and age adjusted values are distinguished by the data_value_type variable, the measureid
#variable does not distinguish between crude and age adjusted variables, we need to distinguish
#between the age adjusted and crude estimates before turning measureid values into columns
#start by simplifying data_value_type, crude is represented by C and adjusted by A
#When I got the right dataset, this step wasn't actually needed

#places_tidy <- places %>%
  #mutate(data_value_type = case_when(
    #data_value_type == "Crude prevalence" ~ "C",
    #data_value_type == "Age-adjusted prevalence" ~ "A"
  #))

#Next, add this to measureid to distinguish between crude and age-adjusted estimates
#places_tidy <- places_tidy %>%
  #mutate(measureid = str_c(measureid, data_value_type))

#creating tidy dataset
places_tidy <- places

#Filtering observations to 2020 only
places_tidy <- places %>%
  filter(year == 2020)

#There are a number of columns that are not necessary, they need to be dropped as well
places_tidy <- places_tidy %>%
  dplyr::select(-(c(statedesc, datasource, category, measure, data_value_unit,
          data_value_type, low_confidence_limit, high_confidence_limit,
          categoryid, datavaluetypeid, short_question_text, countyfips, 
          locationname, data_value_footnote, data_value_footnote_symbol,
          )))

#Finally, we can pivot this dataset to wide format
places_tidy <- places_tidy %>%
  pivot_wider(names_from = "measureid",
              values_from = "data_value")

# merge the data frames by census tract
#swapped places and fara so that the combined dataset only has census tracts that are in fara
combined <- right_join(places_tidy, fara,
                      by = "census_tract") 

# Read in the shapefile
census <- st_read(dsn = "cb_2020_us_county_20m.shp")

#binding census shapefile to this dataset
census <- census %>%
  st_set_crs(value = 4326) 


```

## Exploratory Data Analysis

Use EDA to to understand the distribution of the variables and identify patterns or relationships. Use things like scatter plots or correlation matrices to visualise the relationship between obesity prevalence and other factors like food access, income, or education

```{r}
glimpse(combined)

#A ton of our numeric variables are stored as character variables, they need to be converted to numeric variables
to_convert <- colnames(select_if(combined, is.character))
to_convert <- to_convert[!to_convert %in% c("census_tract", "countyname", "year", "stateabbr", "geometry")]
to_convert <- unlist(to_convert)
combined <- combined %>%
  mutate_at(to_convert, as.numeric)

#some of these numerics need to be converted to factors
factors <- c("urban", "group_quarters_flag", "lila_tracts_1and10", "lila_tracts_half_and10",
            "lila_tracts_vehicle", "hunv_flag", "low_income_tracts",
             "la1and20", "la_tracts_half", "la_tracts1", "la_tracts10",
             "la_tracts20", "la_tracts_vehicle_20")
combined <- combined %>%
  mutate_at(factors, as.factor)

eda_point <- function(data, x, y, fill){
  
  ggplot(data = data, 
       aes(x = {{ x }},
           y = {{ y }})) +
  geom_point(aes(fill = {{ fill }}),
             color = "white", 
             shape = 21) 
}
combined %>%
  st_transform(4326)


eda_geo <- function(data, fill){
  
  ggplot(data = data) +
  geom_sf(aes(fill = {{ fill }})) +
  theme_void() 
}

#point plot to compare obesity prevalence between urban and rural areas
eda_point(combined, poverty_rate, OBESITY, urban)

#boxplot to compare the distribution of obesity prevalence between urban and rural areas
ggplot(combined, aes(x = urban, y = OBESITY)) +
  geom_boxplot() +
  labs(x = "Urban/Rural", y = "Obesity Prevalence")

#While the average obesity rate is higher in rural areas, urban areas have higher variability of of data.

#scatter plot to explore the relationship between obesity prevalence and poverty rate
ggplot(combined, aes(x = poverty_rate, y = OBESITY)) +
  geom_point() +
  labs(x = "Poverty Rate", y = "Obesity Prevalence")

#Unsurprisingly, as the poverty rate increases, obesity rates increase as well.

#chloropleth map of obesity - need to set the data
eda_geo(combined, OBESITY)

#I did some exploration by state. I know we discussed sticking with census tract, but I thought this might be an interesting talking point when we are complete with the census tract analysis. 

# Calculate summary statistics by state
summary_stats <- combined %>%
  group_by(stateabbr) %>%
  summarize(mean_obesity = mean(OBESITY, na.rm = TRUE),
            median_income = median(median_family_income, na.rm = TRUE),
            sd_food_access = sd(la1and10, na.rm = TRUE))


head(summary_stats)

# Plot mean obesity by state
ggplot(summary_stats, aes(x = stateabbr, y = mean_obesity)) +
  geom_bar(stat = "identity") +
  labs(x = "State", y = "Mean Obesity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Plot median income by state
ggplot(summary_stats, aes(x = stateabbr, y = median_income)) +
  geom_bar(stat = "identity") +
  labs(x = "State", y = "Median Income") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Plot standard deviation of food access by state
ggplot(summary_stats, aes(x = stateabbr, y = sd_food_access)) +
  geom_bar(stat = "identity") +
  labs(x = "State", y = "Standard Deviation of Food Access") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

The standard deviation of food access by state will tell us the degree of variability of the food access variable across different census tracts within each state. A higher standard deviation indicates that the food access variable varies widely across census tracts within a state, while a lower standard deviation suggests that the variable is more consistent across census tracts. This information can be useful in identifying states or regions where food access is more or less uniform, and can help in identifying areas where interventions to improve food access may be most needed. 


## (Supervised) Machine Learning

*Setting up the testing environment*

```{r}
# separate geometry column into longitude and latitude and prepare it for recipe 

combinedsmall <- combined %>%
  select(census_tract, COPD, OBESITY, STROKE, DEPRESSION, LPA, CASTHMA, MAMMOUSE, TEETHLOST, ARTHRITIS, COREM, DIABETES, BINGE, SLEEP, ACCESS2, PHLTH, DENTAL, MHLTH, CANCER, CHD, GHLTH, CHECKUP, CSMOKING, CERVICAL, KIDNEY, COLON_SCREEN, COREW, urban, median_family_income, la1and10, lalowi1share, lakids1share, laseniors1share, lahunv1share)

combinedsep <- combinedsmall %>%
  mutate(longitude = st_coordinates(geometry)[, "X"],
         latitude = st_coordinates(geometry)[, "Y"]) 

# to remove the geometry column from dataframe 
combinedsep <- st_drop_geometry(combinedsep)

# convert all variables to numeric
combinedsep$urban <- as.numeric(as.character(combinedsep$urban))

# set seed
set.seed(20230507)

# split the data into training and testing sets 
obesity_split <- initial_split(data = combinedsep, 
                               prop = 0.8)

obesity_train <- training(x = obesity_split)
obesity_test <- testing(x = obesity_split)

# set up v-fold cross validation 
folds <- vfold_cv(data = obesity_train, v = 5, repeats = 1)

# create a recipe (need to look at what needs to be added)
obesity_rec <- recipe(OBESITY ~., data = obesity_train) %>%
  step_other(census_tract) %>%
  step_naomit(all_predictors()) %>%
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors()) %>%
  step_dummy(census_tract)

```

**Linear Regression**

```{r}

# create a linear regression model using the "lm" package as the engine
lm_mod <- linear_reg() %>%
  set_engine("lm")

# create a workflow with the recipe and linear regression model you've created
lm_wf <- workflow() %>%
  add_recipe(obesity_rec) %>%
  add_model(lm_mod) 

# fit the model by piping your workflow
lm_cv <- lm_wf %>%
  fit_resamples(resamples = folds)

# select the best model based on the "rmse" metric
lm_best <- lm_cv %>%
  select_best("rmse")

# use the finalize_workflow() function with your workflow and the best model 
# to update (or "finalize") your workflow by modifying the line below
lm_final <- finalize_workflow(
  lm_wf,
  parameters = lm_best
)

# fit to the training data and extract coefficients
lm_coefs <- lm_final %>%
  fit(data = obesity_train) %>%
  extract_fit_parsnip() %>%
  vi(lambda = lasso_best$penalty)

```

**LASSO Model**

```{r}
# create a tuning grid for lasso regularization, varying the regularization penalty
lasso_grid <- grid_regular(penalty(), levels = 10)

# create a linear_regression model so that you can tune the penalty parameter
# set the mixture parameter to 1 and use "glmnet" for the engine
lasso_mod <- linear_reg(
  penalty = tune(), 
  mixture = 1
) %>%
  set_engine("glmnet")

# create a workflow using your updated linear regression model you just created and the same recipe
# you defined above
lasso_wf <- workflow() %>%
  add_recipe(obesity_rec) %>%
  add_model(lasso_mod) 

# perform hyperparameter tuning using the lasso_grid and the 
# cross_validation folds you created above by modifying the line below
lasso_cv <- lasso_wf %>%
  tune_grid(
    resamples = folds,
    grid = lasso_grid
  )

# select the best model based on the "rmse" metric
lasso_best <- lasso_cv %>%
  select_best(metric = "rmse")

# use the finalize_workflow() function with your lasso workflow and the best model 
# to update (or "finalize") your workflow by modifying the line below
lasso_final <- finalize_workflow(
  lasso_wf,
  parameters = lasso_best
)

# fit to the training data and extract coefficients
lasso_coefs <- lasso_final %>%
  fit(data = obesity_train) %>%
  extract_fit_parsnip() %>%
  vi(lambda = lasso_best$penalty) 

```

**Ridge Model**

```{r}

# create a tuning grid for ridge regularization, varying the regularization penalty
ridge_grid <- grid_regular(penalty(), levels = 10)

# create a linear_regression model so that you can tune the penalty parameter
ridge_mod <- linear_reg(
  penalty = tune(), 
  mixture = 0
) %>%
  set_engine("glmnet")

# create a ridge workflow using your updated linear regression model you just created and 
# the same recipe you defined above
ridge_wf <- workflow() %>%
  add_recipe(obesity_rec) %>%
  add_model(ridge_mod)

# perform hyperparameter tuning using the on your ridge hyperparameter grid and
# cross_validation folds you created above by modifying the line below
# use the tune_grid() function
ridge_cv <- ridge_wf %>%
  tune_grid(
    resamples = folds,
    grid = ridge_grid
  )

# select the best model based on the "rmse" metric
ridge_best <- ridge_cv %>%
  select_best(metric = "rmse")

# use the finalize_workflow() function with your ridge workflow and the best model 
# to update (or "finalize") your workflow
ridge_final <- finalize_workflow(
  ridge_wf,
  parameters = ridge_best
)

# fit the final ridge model to the full training data and extract coefficients
# by updating the line below
ridge_coefs <- ridge_final %>%
  fit(data = obesity_train) %>%
  extract_fit_parsnip() %>%
  vi(lambda = ridge_best$penalty) 

```

**Elastic Net**

```{r}
# create a tuning grid for elastic net regularization, varying the regularization penalty
elastic_net_grid <- grid_regular(penalty(), levels = 10)

# create a linear_regression model so that you can tune the penalty parameter
# and use "glmnet" for the engine. If you set mixture = 1 for lasso regression above,
# and mixture = 0 for ridge regression, what should you set mixture equal to for elastic net?
elastic_net_mod <- linear_reg(
  penalty = tune(), 
  mixture = 0.5) %>%
  set_engine("glmnet")

# create an elastic net workflow using your updated linear regression model you just created and 
# the same recipe you defined above
elastic_net_wf <- workflow() %>%
  add_recipe(obesity_rec) %>%
  add_model(elastic_net_mod)

# perform hyperparameter tuning using the on your elastic net hyperparameter grid and
# cross_validation folds you created above by modifying the line below
elastic_net_cv <- elastic_net_wf %>%
  tune_grid(
    resamples = folds,
    grid = elastic_net_grid)

# select the best model based on the "rmse" metric
elastic_net_best <- elastic_net_cv %>%
  select_best(metric = "rmse")

# use the finalize_workflow() function with your elastic net workflow and the best model 
# to update (or "finalize") your workflow
elastic_net_final <- finalize_workflow(
  elastic_net_wf,
  parameters = elastic_net_best)

# fit the final elastic net model to the full training data and extract coefficients
# by updating the line below
elastic_net_coefs <- elastic_net_final %>%
  fit(data = obesity_train) %>%
  extract_fit_parsnip() %>%
  vi(lambda = elastic_net_best$penalty)

```

## Compare Models

```{r}
# the models are comparable for prediction accuracy
bind_rows(
  `lm` = show_best(lm_cv, metric = "rmse", n = 1),
  `LASSO` = show_best(lasso_cv, metric = "rmse", n = 1),
  `ridge` = show_best(ridge_cv, metric = "rmse",n = 1),
  `enet` = show_best(elastic_net_cv, metric = "rmse", n = 1),
  .id = "model"
)

all_coefs <- bind_rows(
  `lm` = lm_coefs,
  `LASSO` = lasso_coefs,
  `ridge` = ridge_coefs,
  `enet` = elastic_net_coefs,
  .id = "model"
) 

all_coefs %>%
  group_by(model) %>%
  slice_max(Importance, n = 10) %>%
  ggplot(aes(Importance, Variable, fill = model)) +
  geom_col(position = "dodge")

all_coefs %>%
  filter(model != "lm") %>%
  group_by(model) %>%
  slice_max(Importance, n = 10) %>%
  ggplot(aes(Importance, Variable, fill = model)) +
  geom_col(position = "dodge")


# compare the regularized coefficients to the lm coefficients for all three models
plot1 <- left_join(
  rename(lm_coefs, lm = Importance),
  rename(lasso_coefs, LASSO = Importance),
  by = "Variable"
) %>%
  ggplot(aes(lm, LASSO)) +
  geom_point(alpha = 0.3)

plot2 <- left_join(
  rename(lm_coefs, lm = Importance),
  rename(ridge_coefs, ridge = Importance),
  by = "Variable"
) %>%
  ggplot(aes(lm, ridge)) +
  geom_point(alpha = 0.3)

plot3 <- left_join(
  rename(lm_coefs, lm = Importance),
  rename(elastic_net_coefs, enet = Importance),
  by = "Variable") %>%
  ggplot(aes(lm, enet)) +
  geom_point(alpha = 0.3)

plot1 + plot2 + plot3
```

**Decision Tree Model**

```{r}
# create a model
tree_mod <- 
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")

# create a workflow
tree_wf <- workflow() %>%
  add_recipe(obesity_rec) %>%
  add_model(tree_mod)

# resampling 
tree_cv <- tree_wf %>%
  fit_resamples(resamples = folds)

# use RMSE to choose the best model 
tree_best <- tree_cv %>%
  select_best(metric = "rmse")

# finalise workflow 
tree_final <- tree_wf %>%
  finalize_workflow(parameters = tree_best)

# fit model to the training set 
tree_fit <- tree_wf %>%
  fit(data = obesity_train)

# create a tree 
rpart.plot::rpart.plot(x = obesity_fit$fit$fit$fit, roundint = FALSE)

# making predictions on the test dataset
predictions <- bind_cols(
  obesity_test,
  predict(object = obesity_fit, new_data = obesity_test),
  predict(object = obesity_fit, new_data = obesity_test, type = "prob")
)

#comparing tree predictions to true values
conf_mat(data = predictions, 
         truth = grade,
         estimate = .pred_class)

#calculating accuracy, recall, and precision
accuracy <- accuracy(data = predictions,
                     truth = grade,
                     estimate = .pred_class)


precision <- yardstick::precision(data = predictions,
                                  truth = grade,
                                  estimate = .pred_class)

recall <- yardstick::recall(data = predictions,
                            truth = grade,
                            estimate = .pred_class)

print(c("the model accuracy is:", round(accuracy$.estimate, digits = 3)))
print(c("the model precision is:", round(precision$.estimate, digits = 3)))

print(c("the model recall is:", round(recall$.estimate, digits = 3)))


# variable importance

tree_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 10)

```

## Final Results 

```{r}
library(tidyverse)
library(skimr)




```

## Conclusion 