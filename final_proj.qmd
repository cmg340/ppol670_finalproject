---
title: "Final Project - Predicting Health Indicators"
author: "Elena Spielmann, Patrick Jones, Leanne Chook, and Maeve Grady"
format: 
  html:
    code-fold: true
    self-contained: true
execute: 
  echo: true
  warning: false
  error: false
editor_options: 
  chunk_output_type: console
---
## Introduction 

Obesity has reached epidemic proportions in the United States, and affects people of all ages, socioeconomic backgrounds and ethnicities, which imposes substantial economic burdens in the form of productivity losses, strains to the healthcare system and costs. 

The prevalence of obesity in the United States was approximately 41.9% in 2017 to 2020, of that, the prevalence of severe obesity also increased to 9.2%. This increasing trend is worrying because obesity can lead to several other health conditions such as heart disease, strokes, type 2 diabetes, and certain types of cancers, all of which are the leading causes of preventable and premature deaths in the country. 

Furthermore, this increasing trend of obesity also has other implications on national security, especially as more individuals become ineligible for the military, therefore, significantly reducing military recruitment numbers. 

**Motivating question:**
The main motivating question for this project was to examine obesity rates across the United States and find predictors that can help policymakers make effective decisions to help reduce obesity prevalence in the most vulnerable areas. 

**Outline of project:**
This project will be as follows, first we will examine and clean up the available information from datasets downloaded from PLACES and the Food Access Repository Atlas. Then we will conduct exploratory data analysis (EDA) and geospatial analysis to guide our decision for the final models, and finally we create several predictive models to find the best model with highest predictive power. 

*Setup (loading packages)*

```{r}
#install.packages("httr")
library(httr)
library(sf)
#install.packages("tmap")
library(tmap)
library(dotenv)
library(here)
library(readxl)
library(dplyr)
library(tidyverse)
library(tidymodels)
library(themis)
library(rpart.plot)
library(vip)
library(lubridate)
library(rpart)
library(ranger)
library(ggplot2)
library(parsnip)
library(yardstick)
library(sf)
library(janitor) 
library(stringr)
library(RSocrata)
library(tidycensus)
library(tigris)
```

## Reading in Data using APIs

```{r}
#accessing FARA data through ArcGIS REST API
## to write this code I consulted this blog post: https://community.esri.com/t5/gis-blog/accessing-arcgis-rest-services-using-r/ba-p/898451

url <- parse_url("https://gis.ers.usda.gov/arcgis/rest/services")
url$path <- paste(url$path, "foodaccess2019/MapServer/0/query", sep = "/")
url$query <- list(returnGeometry = "true",
                  f = "geojson",
                  outFields = "*")
request <- build_url(url)

foodaccess <- st_read(request) ## this should create a full dataframe but is obviously not working right now.  


##using this api to try to figure out how the query should work:
##https://gis.ers.usda.gov/arcgis/rest/services/foodaccess2019/MapServer/0/query?where=&text=&objectIds=&time=&geometry=&geometryType=esriGeometryPolygon&inSR=&spatialRel=esriSpatialRelIntersects&distance=&units=esriSRUnit_Foot&relationParam=&outFields=*&returnGeometry=true&returnTrueCurves=false&maxAllowableOffset=&geometryPrecision=&outSR=&havingClause=&returnIdsOnly=false&returnCountOnly=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&returnZ=false&returnM=false&gdbVersion=&historicMoment=&returnDistinctValues=false&resultOffset=&resultRecordCount=&returnExtentOnly=false&datumTransformation=&parameterValues=&rangeValues=&quantizationParameters=&featureEncoding=esriDefault&f=html

```

```{r}
#install.packages("RSocrata")
library(RSocrata) ## the Rsocrata package has to be loaded in after httr package for the above code to work 

##reading in api credentials
load_dot_env(here(".env"))
app_token <- Sys.getenv("PLACES_app_token")
user_name <- Sys.getenv("PLACES_username")
password <- Sys.getenv("PLACES_password")


#PLACES dataset

places <- read.socrata(
  "https://chronicdata.cdc.gov/resource/swc5-untb.json",
  app_token = paste(app_token),
  email     = paste(user_name),
  password  = paste(password)
)
```

## Combining the Datasets

**The Food Access Research Atlas (FARA)** provides a variety of food access measures for low income and low access census tracts. Food access measures use income, transportation, and distance from grocery stores and other food sellers to determine how accessible food is to residents of a given census tract. This dataset includes distinct measures of access for urban and rural populations.

(rewrite this)
**PLACES** is an expansion of the original 500 Cities Project that began in 2015. The original project was launched by the Centers for Disease Control and Prevention (CDC) in partnership with the Robert Wood Johnson Foundation (RWJF) and CDC Foundation. In 2018, this partnership was extended through 2020. In 2020, the project expanded to provide small area estimates (SAE) for counties, places, census tracts, and ZIP Code Tabulation Areas (ZCTA) across the entire United States.

```{r}
# load the FARA dataset 
fara <- read_excel("fara_2019.xlsx")

# clean the variable names 
fara <- clean_names(fara)


# load the PLACES dataset
places <- st_read("PLACES_ Local Data for Better Health, Census Tract Data 2022 release.geojson")%>% st_transform(value = 4326)

places <- places %>%
  rename(census_tract = locationid)

#The places dataset must be made tidy in order to properly merge it with FARA
#Column names will come from measureid, and values from data_value
#Crude and age adjusted values are distinguished by the data_value_type variable, the measureid
#variable does not distinguish between crude and age adjusted variables, we need to distinguish
#between the age adjusted and crude estimates before turning measureid values into columns
#start by simplifying data_value_type, crude is represented by C and adjusted by A
#When I got the right dataset, this step wasn't actually needed
#places_tidy <- places %>%
  #mutate(data_value_type = case_when(
    #data_value_type == "Crude prevalence" ~ "C",
    #data_value_type == "Age-adjusted prevalence" ~ "A"
  #))
#Next, add this to measureid to distinguish between crude and age-adjusted estimates
#places_tidy <- places_tidy %>%
  #mutate(measureid = str_c(measureid, data_value_type))

#creating tidy dataset
places_tidy <- places
#Filtering observations to 2020 only
places_tidy <- places %>%
  filter(year == 2020)
#There are a number of columns that are not necessary, they need to be dropped as well
places_tidy <- places_tidy %>%
  dplyr::select(-(c(statedesc, datasource, category, measure, data_value_unit,
          data_value_type, low_confidence_limit, high_confidence_limit,
          categoryid, datavaluetypeid, short_question_text, countyfips, 
          locationname, data_value_footnote, data_value_footnote_symbol,
          )))
#Finally, we can pivot this dataset to wide format
places_tidy <- places_tidy %>%
  pivot_wider(names_from = "measureid",
              values_from = "data_value")
# merge the data frames by census tract
#swapped places and fara so that the combined dataset only has census tracts that are in fara
combined <- right_join(places_tidy, fara,
                      by = "census_tract")

# Read in the shapefile - opting for tidy census here
load_dot_env(here(".env"))


census_api_key("census_api_key", install = T, overwrite=TRUE)

options(tigris_use_cache = TRUE)

allstates <- c(state.abb)
allstates

census <- get_acs(state = allstates ,
                  geography = "tract", 
                  variables =  "B19013_001",
                  geometry = TRUE,
                  year = 2019)%>%
  rename(census_tract = GEOID)%>%
  st_transform(crs = st_crs(4326))
  


#binding census shapefile to this dataset
combined_nonsf <- combined %>% st_transform(crs = st_crs(4326))
combined_census <- st_join(census, combined_nonsf, join = st_intersects)


```

## Exploratory Data Analysis

Use EDA to to understand the distribution of the variables and identify patterns or relationships. Use things like scatter plots or correlation matrices to visualize the relationship between obesity prevalence and other factors like food access, income, or education

```{r}
glimpse(combined)
#A ton of our numeric variables are stored as character variables, they need to be converted to numeric variables
to_convert <- colnames(select_if(combined, is.character))
to_convert <- to_convert[!to_convert %in% c("census_tract", "countyname", "year", "stateabbr", "geometry")]
to_convert <- unlist(to_convert)
combined <- combined %>%
  mutate_at(to_convert, as.numeric)
#some of these numerics need to be converted to factors
factors <- c("urban", "group_quarters_flag", "lila_tracts_1and10", "lila_tracts_half_and10",
            "lila_tracts_vehicle", "hunv_flag", "low_income_tracts",
             "la1and20", "la_tracts_half", "la_tracts1", "la_tracts10",
             "la_tracts20", "la_tracts_vehicle_20")
combined <- combined %>%
  mutate_at(factors, as.factor)

eda_point <- function(data, x, y, fill){
  
  ggplot(data = data, 
       aes(x = {{ x }},
           y = {{ y }})) +
  geom_point(aes(fill = {{ fill }}),
             color = "white", 
             shape = 21) 
}
combined %>%
  st_transform(4326)


eda_geo <- function(data, fill){
  
  ggplot(data = data) +
  geom_sf(aes(fill = {{ fill }})) +
  theme_void() 
}

#point plot to compare obesity prevalence between urban and rural areas
eda_point(combined, poverty_rate, OBESITY, urban)

#boxplot to compare the distribution of obesity prevalence between urban and rural areas
ggplot(combined, aes(x = urban, y = OBESITY)) +
  geom_boxplot() +
  labs(x = "Urban/Rural", y = "Obesity Prevalence")
#While the average obesity rate is higher in rural areas, urban areas have higher variability of of data.

#scatter plot to explore the relationship between obesity prevalence and poverty rate
ggplot(combined, aes(x = poverty_rate, y = OBESITY)) +
  geom_point() +
  labs(x = "Poverty Rate", y = "Obesity Prevalence")
#Unsurprisingly, as the poverty rate increases, obesity rates increase as well.

#chloropleth map of obesity - need to set the data
eda_geo(combined, OBESITY)

#I did some exploration by state. I know we discussed sticking with census tract, but I thought this might be an interesting talking point when we are complete with the census tract analysis. 

# Calculate summary statistics by state
summary_stats <- combined %>%
  group_by(stateabbr) %>%
  summarize(mean_obesity = mean(OBESITY, na.rm = TRUE),
            median_income = median(median_family_income, na.rm = TRUE),
            sd_food_access = sd(la1and10, na.rm = TRUE))


head(summary_stats)

# Plot mean obesity by state
ggplot(summary_stats, aes(x = stateabbr, y = mean_obesity)) +
  geom_bar(stat = "identity") +
  labs(x = "State", y = "Mean Obesity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Plot median income by state
ggplot(summary_stats, aes(x = stateabbr, y = median_income)) +
  geom_bar(stat = "identity") +
  labs(x = "State", y = "Median Income") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Plot standard deviation of food access by state
ggplot(summary_stats, aes(x = stateabbr, y = sd_food_access)) +
  geom_bar(stat = "identity") +
  labs(x = "State", y = "Standard Deviation of Food Access") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

#The standard deviation of food access by state will tell us the degree of variability of the food access variable across different census tracts within each state. A higher standard deviation indicates that the food access variable varies widely across census tracts within a state, while a lower standard deviation suggests that the variable is more consistent across census tracts. This information can be useful in identifying states or regions where food access is more or less uniform, and can help in identifying areas where interventions to improve food access may be most needed. 

```

```{r}
glimpse(combined_census)
combined_census <- combined_census %>% clean_names()%>% filter()

# Bin the number of homicides into groups 
breaks <- c(0, 10, 20, 30, 40, 50, 60, 70)

combined_census <- combined_census %>%
  mutate(obesity_num = as.numeric(obesity),
         obesity_bin = cut(obesity_num, breaks = 10))
  


obesity_tracts <-  ggplot()+
  geom_sf(data = combined_census, 
          aes(fill = obesity_bin),
          lwd = 0,
          color = NA)+
  labs(title = "Adult obesity rate by census tract in the US")+
  coord_sf(xlim=c(-160, -64),ylim=c(18,72), crs = 4326) +
  theme_void()

obesity_tracts
obesity_states
```

## (Supervised) Machine Learning

*Setting up the testing environment*

```{r}
set.seed(20230507)

# split the data into training and testing sets 
obesity_split <- initial_split(data = combined, 
                               prop - 0.8)

obesity_train <- training(x = obesity_split)
obesity_test <- testing(x = obesity_split)

# set up v-fold cross validation 
folds <- vfold_cv(data = obesity_train, v = 10, repeats = 1)

# create a recipe (need to look at what needs to be added)
obesity_rec <- recipe(formula = , data = obesity_train) %>%
  step_naomit(all_predictors()) %>%
  step_center(all_predictors()) %>%
  step_nzv(all_predictors()) %>%


```

**Linear Regression (or any other) Model**

```{r}

# create model
lm_mod <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode(mode = "regression")

# set up workflow 
lm_wf <- workflow() %>%
  add_recipe(obesity_rec) %>%
  add_model(lm_mod)

# use resamples 
lm_cv <- lm_wf %>%
  fit_resamples(resamples = folds,
                metrics = metric_set(rmse, mae))

# collect metrics and find the best model
lm_best <- lm_cv %>%
  select_best(metric = "rmse")

lm_final <- lm_wf %>%
  finalize_workflow(parameters = lm_best)

collect_metrics(lm_cv, summarize = TRUE)

# fit the model
lm_fit <- lm_final %>%
  fit(obesity_train)

# evaluate its performance using MSE or accuracy

collect_metrics(lm_cv, summarize = FALSE) %>%
  filter(.metric == "rmse") %>%
  ggplot(mapping = aes(x = id, y = .estimate, group = .estimator, label = round(.estimate, digits = 2))) +
  geom_line() +
  geom_point() +
  geom_label(nudge_y = -500) +
  scale_y_continuous(limits = c(10000, 17500)) +
  labs(title = "Obesity: Linear Regression Model (RMSE Across 10 Folds)", 
       y = "Predicted RMSE",
       x = "Fold Number") +
  theme_minimal()

```

**KNN Model**

```{r}
knn_mod <- nearest_neighbor(neighbors = tune()) %>%
  set_engine(engine = "kknn") %>%
  set_mode(mode = "regression")

# set up workflow
knn_workflow <- 
  workflow() %>%
  add_model(spec = knn_mod) %>%
  add_recipe(recipe = obesity_rec)

# create a tuning grid 
knn_grid <- grid_regular(neighbors(range = c(1, 15)), levels = 8)

# fitting model to the training data 
knn_res <- 
  knn_workflow %>%
  tune_grid(resamples = folds,
            grid = knn_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse, mae))

# collecting the metrics and determining the best model
knn_best <- knn_res %>%
  select_best(metric = "rmse")

# finalize workflow 
knn_final <- knn_workflow %>%
  finalize_workflow(parameters = knn_best)

# fit model 
knn_fit <- knn_final %>%
  fit(obesity_train)

knn_fit_rs <- knn_final %>%
  fit_resamples(resamples = folds)

# evaluate the model
collect_metrics(knn_res, summarize = FALSE) %>%
  filter(.metric == "rmse") %>%
  ggplot(mapping = aes(x = id, y = .estimate, group = .estimator, label = round(.estimate, digits = 2))) +
  geom_line() +
  geom_point() +
  geom_label(nudge_y = -500) +
  scale_y_continuous(limits = c(10000, 17500)) +
  labs(title = "Obesity: KNN Model (RMSE Across 10 Folds)", 
       y = "Predicted RMSE",
       x = "Fold Number") +
  theme_minimal()


```

**Decision Tree Model**

```{r}
# create a model
tree_mod <- 
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")

# create a workflow
tree_wf <- workflow() %>%
  add_recipe(obesity_rec) %>%
  add_model(tree_mod)

# resampling 
tree_cv <- tree_wf %>%
  fit_resamples(resamples = folds)

# use RMSE to choose the best model 
tree_best <- tree_cv %>%
  select_best(metric = "rmse")

# finalise workflow 
tree_final <- tree_wf %>%
  finalize_workflow(parameters = tree_best)

# fit model to the training set 
tree_fit <- tree_wf %>%
  fit(data = obesity_train)

# create a tree 
rpart.plot::rpart.plot(x = obesity_fit$fit$fit$fit, roundint = FALSE)

# making predictions on the test dataset
predictions <- bind_cols(
  obesity_test,
  predict(object = obesity_fit, new_data = obesity_test),
  predict(object = obesity_fit, new_data = obesity_test, type = "prob")
)

#comparing tree predictions to true values
conf_mat(data = predictions, 
         truth = grade,
         estimate = .pred_class)

#calculating accuracy, recall, and precision
accuracy <- accuracy(data = predictions,
                     truth = grade,
                     estimate = .pred_class)


precision <- yardstick::precision(data = predictions,
                                  truth = grade,
                                  estimate = .pred_class)

recall <- yardstick::recall(data = predictions,
                            truth = grade,
                            estimate = .pred_class)

print(c("the model accuracy is:", round(accuracy$.estimate, digits = 3)))
print(c("the model precision is:", round(precision$.estimate, digits = 3)))

print(c("the model recall is:", round(recall$.estimate, digits = 3)))


# variable importance

tree_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 10)

```

## Final Results 

```{r}
library(tidyverse)
library(skimr)




```

## Conclusion 